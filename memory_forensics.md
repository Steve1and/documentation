# Memory Forensics Explanation and Techniques
***

## General Concepts

The underlying hardware of a PC ultimately dictates the constraints of what a particular system can do. As such, it is important to understand the common components in each PC's architecture as a foundation to aid in determining the possible cause and effect of suspected malicious activity. Below is a list of various components found in most PC's with a brief description of each.

- **Motherboard**: Also known as the *mainboard*, it provides the connections that enable the components of the system to communicate.
- **Busses**: Communication channels between devices, often integrated directly into the Motherboard.
- **CPU and MMU**: The Central Processing Unit (CPU), or just processor, accesses main memory to obtain its instructions and then executes those instructions to process data and execute programs. CPU's also typically have a small form of memory known as *cache* to help offset the disparity in accessing speed when retrieving instructions from main memory. The CPU relies on its Memory Management Unit (MMU) to help find where the data is stored. The MMU is the hardware unit that translates the address that the processor requests to its corresponding address in main memory. The CPU also has a special cache, called the Translation Lookaside Buffer or (TLB), that is consulted before the MMU to reduce the need to access Main Memory.
- **Memory Controller**: Responsible for mediating potentially concurrent requests for system memory from the processor and devices.
- **Direct Memory Access (DMA)**: Provides I/O devices the capability to directly transfer data stored in system memory without processor intervention.
  - In a forensics context, the DMA provides a mechanism to directly access the contents of physical memory from a peripheral device without involving the un-trusted software running on the machine.
- **Volatile Memory (RAM)**: The main memory of a PC which stores the code and data that the processor actively accesses and stores.


#### Address Spaces
For a CPU to execute instructions and access data stored in main memory, it must specify a unique address for that data. *Address Space* refers to a range of valid addresses used to identify the data stored within a finite allocation of memory. The typical addressing scheme generally starts with byte 0 and ends at the offset of the final byte of memory in the allocation. The single continuous address space that is exposed to a running program is referred to as a *linear address space*.
  - When dealing with raw, padded memory dumps, a physical address is essentially an offset into the memory dump file.


#### Intel IA-32 Architecture
The IA-32 architecture commonly refers to the family of x86 architectures that support 32-bit computation. It is a *little-endian* (least-significant bit first) machine that uses byte-addressing. Software running on an IA-32 processor can have a linear address space and a physical address space up to 4GB.

**Registers**
Registers are a small amount of extremely fast memory defined in the IA-32 architecture that the CPU uses for temporary storage during processing. Each processor core contains eight 32-bit general-purpose registers that control the processor's behavior.
  - One of the most significant of these registers is the `EIP` register, that contains the linear address of the next instruction that executes.
  - The IA-32 architecture also has five control registers that specify the configuration of the processor and the characteristics of the executing task.
    - `CR0`: contains flags that control the operating mode of the processor, including a flag that enables paging.
    - `CR1`: reserved and should not be accessed.
    - `CR2`: contains the linear address that caused a page fault.
    - `CR3`: contains the physical address of the initial structure used for address translation. It is updated during context switches when a new task is scheduled.
    - `CR4`: used to enable architectural extensions, including *Physical Address Extension (PAE)*, which is a feature that can expand the size of physical memory to 64GB.
  - **Physical Address Extension (PAE)**: a paging mechanism that allows a 32 bit processor to support physical address spaces greater than 4GB. On systems with PAE enabled, the linear address is divided into four indexes: *Page Directory Pointer Table (PDPT)*, *Page Directory (PD)*, *Page Table (PT)*, and *Page Offset*.
IA-32 processors implement two memory management mechanisms: *segmentation* and *paging*.
- **Segmentation**: divides the 32-bit linear address space into multiple variable-length segments. All IA-32 memory references are addressed using a 16-bit segment selector, which identifies a particular segment descriptor, and a 32-bit offset into the specified segment. A segment descriptor is a memory-resident data structure that defines the location, size, type, and permissions for a given segment.
  - Each processor core contains two special registers, `GDTR` and `LDTR`, which point to tables of segment descriptors, called the *Global Descriptor Table (GDT)* and the *Local Descriptor Table*, respectively.
  - The segmentation registers `CS` (for code), `SS` (for stack), and `DS`, `ES`, `FS`, and `GS` (each for data) should always contain valid segment selectors.
- **Paging**: provides the ability to virtualize the linear address space. It creates an execution environment in which a large linear address space is simulated with a modest amount of physical memory and disk storage. Each 32-bit linear address space is broken up into fixed-length sections, called *pages*, which can be mapped into physical memory in an arbitrary order.
  - When a program attempts to access a linear address, this mapping uses memory-resident *page directories* and *page tables* to translate the linear address into a physical address.

#### Intel IA-64 Architecture
The execution environment for the Intel 64 architecture is similar to IA-32, however, the registers have been extended to hold 64 bits and the processor is now able to support 64-bit linear addresses. Simply put, this means that Intel 64 architecture supports a linear address space up to 2^64 bytes. A new feature that matters to forensics tools is the additional level of paging structures called *page map level 4 (PML4)*. All entries in the hierarchy of paging structures are 64 bits, and they can map virtual addresses to pages of size 4KB, 2MB, or 1GB.

#### Interrupt Descriptor Table
PC architectures often provide a mechanism for interrupting process execution and passing control to a privileged mode software routine. For the IA-32 and Intel 64 architectures, the routines are stored within the *Interrupt Descriptor Table (IDT)*. Each processor has its own IDT composed of 256 8-byte or 16-byte entries, in which the first 32 entries are reserved for processor-defined exceptions and interrupts. Each entry contains the address of the *interrupt service routine (ISR)* that can handle the particular interrupt or exception. In the event of an interrupt or exception, the specified interrupt number serves as an index into the IDT (which indirectly references a segment in the GDT), and the CPU will call the respective handler. Operating systems also use the IDT to store handlers for numerous other events, including system calls, debugger breakpoints, and other faults. In a forensics case, the IDT provides some interesting artifacts when something such as a rootkit might try to redirect entries, modify handler code, add new entries, or even create entirely new interrupt tables. For example, Shadow Walker hooked the page fault handler by modifying the IDT and was able to return "fake" pages to the caller. Therefore, the IDT can be a good place to look for possible malicious programs attempting to gain control of the processor and thereby achieve execution, typically with an IDT entry that that contains ISR entries mapping to unknown segments of memory, likely where malware is hiding.

### Operating Systems
This section details specifics of particular OS's as they might impact memory forensics.

**Privilege Separation**
To prevent possible malicious programs from compromising critical components of the OS, most modern operating systems implement some form of user and kernel mode privilege isolation. This separation is enforced through the use of four privilege levels commonly referred to as *protection rings*. In most operating systems, kernel mode is implemented in *ring 0* (most privileged) and user mode in *ring 3* (least privileged).

**System Calls**
A user application can request a service from the operating system's kernel using a system call. System calls define the low-level API between user applications and the operating system kernel. For example, in Windows system calls are routed through API's provided by `ntdll.dll` and `kernel32.dll`. To invoke a system call, an application executes a software interrupt or architecture-specific instruction, which saves the user mode register context, changes the execution mode to kernel, initializes the kernel stack, and invokes the system call handler. After the request is serviced, execution is returned to user mode and the unprivileged register context is restored. Control is then returned to the instruction following the system call. Because it is such a critical bridge between user applications and the operating system, the code used to service system call interrupts is commonly intercepted by security products and targeted by malicious software.

### Process Management
A process is an instance of a program executing in memory. The OS is responsible for managing process creation, suspension, and termination. When a program executes, a new process is created and associated with its own set of attributes, including a unique process ID and address space, which becomes the container for the applications code, shared libraries, dynamic data, and runtime stack. A process also possesses at least a single thread of execution. A process provides the execution environment, resources, and context for threads to run. An important aspect of memory analysis involves enumerating the processes that were executing on a system and analyzing the data stored within their address spaces, including passwords, URLs, encryption keys, e-mail, and chat logs.

**Threads**: the basic unit of CPU utilization and execution. Often characterized by a thread ID, CPU register set, and execution stack(s), which define the thread's execution context. Despite their unique execution contexts, a process's threads share the same code, data, address space, and operating system resources. In terms of memory forensics, thread data structures are useful because they often contain timestamps and starting addresses. This information can help determine what cod in a process has executed and when it began.

**CPU Scheduling**: The OS's ability to distribute CPU execution time among multiple threads. The scheduler implements policies that govern which threads execute and how long they execute. Switching execution of one thread to another is called a *context switch*. During a context switch, the OS suspends the execution of a thread and stores its execution context in main memory. The OS then retrieves the execution context of another thread from memory, updates the state of the CPU registers, and resumes execution where it was previously suspended. The saved execution context associated with suspended threads can provide valuable insight during memory analysis. For example, it can provide details about which sections of code were being executed or which parameters were passed to system calls.

**System Resources**: An important service an OS provides is helping to manage a process' resources. A process acts as a container for system resources that are accessible to its threads. Most modern operating systems maintain data structures for managing the resources that are actively being accessed, which processes can access them, and how they are accessed. Windows leverages an object manager to supervise the use of system resources and subsequently stores that information in a handle table. A handle provides the process with a unique identifier for accessing and manipulating system resources. It is also used to enforce access control to those resources and track their usage. Linux and Mac both use file descriptors in a similar manner.

### Memory Management
Memory management refers to the OS's algorithms for managing the allocation, de-allocation, and organization of physical memory. These allocations often depend on hardware support.

**Virtual Memory**: Operating systems provide each process with its own private virtual address space. This abstraction creates a separation between the logical memory that a process sees and the actual physical memory installed on the machine. Behind the scenes a memory manager is responsible for transferring regions of memory to secondary storage to free up space in physical memory. During execution, the memory manager and the MMU work together to translate the virtual address into physical addresses. If a thread accesses a virtual address that has been moved to secondary storage, that data is then brought back into physical memory (typically via page fault). With the support of the hardware, the memory manager can partition the data to prevent a malicious process from reading or writing memory that belongs to kernel memory or other processes.

**Demand Paging**: The mechanism that is commonly used to implement virtual memory, which is a memory management policy for determining which regions are resident in main memory and which are moved to a slower secondary storage when the need arises. The most common secondary storage is a file or partition on an internal disk, referred to as the *page file* or *swap*, respectively. A demand paging implementation attempts to load only the pages that are actually needed into memory as opposed to entire processes. Demand paging relies on a characteristic of memory usage known as *locality of reference*, which is based on the observation that memory locations are likely to be frequently accessed in a short period of time, as are their neighbors. An operating system's memory manager often has a mechanism for designating which regions of memory are paged versus those that must remain resident. Demand paging adds some complexity to memory forensics because some pages might not be memory resident at the time the memory sample is collected.

**Shared Memory**: Operating systems provide mechanisms that allow processes to share memory. A common use for shared memory is to provide an efficient means of communication between processes. After a shared region is mapped into virtual address spaces, processes can use the region to exchange messages and data. In situations with shared or dynamic libraries that contain common code and data, the shared pages are typically mapped as *copy-on-write*, which allows the memory manager to defer making a private copy of the data withing a process' address space until the memory has been modified. After the page is written to, the memory manager allocates a private copy of that page with the associated modifications and updates the virtual memory mappings for that process. The other processes are unaffected and still map to the original shared page. In the case of memory forensics, we can often find malicious software attempting to modify the code of shared libraries to hijack the flow of execution.

**Stacks and Heaps**: The *stack* region holds the temporary data associated with executing functions. The data in this region is stored in a data structure called a *stack frame*. Each frame includes information, such as the function parameters, local variables, and the information required to recover the previous stack frame. When a thread is executing, stack frames are stored (pushed) when calling a function and removed (popped) when returning from a function. Because a process can execute in either kernel mode or user mode, operating systems typically use a separate stack for the functions executed within each mode. During malware analysis, stack frames can be used to infer what part of the malware was active and what parts of the system the malware was interacting with.

The *heap* is where application data that needs to be dynamically allocated is stored. Unlike data allocated on the stack, which persists only for the scope of a function, the data allocated within the heap can persist for the lifetime of the process. A heap stores information whose length and contents may not be known at compile time. Applications can allocate memory regions on the heap as they are needed and then deallocate them after use. Since heap data is application-specific, during memory forensics it may be necessary to perform hex dump analysis to determine what was stored at the time of capture.

### File System
Operating systems can use secondary storage to manage persistent data objects that a user wants to access for a time-frame longer than the lifetime of any particular process. The collection of data structures that allow an application to perform primitive operations on the stored data is also known as a *file system*. File system forensics involves finding files or content of interest, recovering file artifacts (deleted, fragments, hidden), and leveraging temporal metadata such as time-stamps to reconstruct the events of an incident.

### I/O Subsystem
The I/O subsystem abstracts the details of peripheral devices and enables a process to communicate with them using a standard set of routines. Many operating systems generalize the interface to devices by treating them as files. Operating systems typically use device driver kernel modules as a mechanism for extending the capabilities of the kernel to support new devices.

**Device Drivers**: abstract away the details of how a device controls and transfers data. Device drivers typically communicate with the registers of the device controller. Operating systems also use device drivers to implement virtual, software-only devices. For example, physical memory is represented in Windows as `\Device\PhysicalMemory`.

**I/O Controls (IOCTLs)**: IOCTL commands are another common mechanism for communicating between user mode and kernel mode. These commands allow a user application to communicate with a kernel mode device driver. They also provide a mechanism for third-party hardware devices and drivers to define their own interfaces and functionality. As with system calls, kernel-level malware might hook IOCTL functions in order to filter results or modify control flow. Malware has also used IOCTL handlers to communicate between user mode and kernel mode components (for example, to request that a kernel component to elevate privileges, disable a service, or modify firewall settings). Memory forensics can detect modified or unknown IOCTLs and provide valuable insight into how attackers leverage them.
***

## Data Structures

This section covers the various ways that data is organized within volatile storage. Understanding these concepts is important for performing memory analysis. If you are familiar with programming concepts then these structures will also be familiar to you and provide a template for interpreting the layout of the data. Many of the data types below are native to the C programming language for reference.

- A note on **pointers**. They are a unique data type that holds a virtual memory address. Programs can declare a pointer to any type of data (i.e., char, long, etc.). To access the stored data, you must de-reference the pointer, which requires virtual address translation. Thus, the inability to translate address will limit the types of analysis you can perform on a physical memory sample.

#### Common Storage Sizes for C Basic Data Types

| Type | 32-Bit Storage Sizes (Bytes) | 64-Bit Storage Size (Bytes) |
| ---- | ---------------------------- | --------------------------- |
| char | 1 | 1 |
| unsigned char | 1 | 1 |
| signed char | 1 | 1 |
| int | 4 | 4 |
| unsigned int | 4 | 4 |
| short | 2 | 2 |
| unsigned short | 2 | 2 |
| long | 4 | Windows: 4, Linux/Mac: 8 |
| unsigned long | 4 | Windows: 4, Linux/Mac: 8 |
| long long | 8 | 8 |
| unsigned long long | 8 | 8 |
| float | 4 | 4 |
| double | 8 | 8 |
| pointer | 4 | 8 |

- Windows also defines many of its own types based on the basic types that can be seen throughout the Windows header files and documentation. These are shown in the table below:

| Type | 32-Bit Size (Bytes) | 64-Bit Size (Bytes) | Purpose/Native Type |
| ---- | ------------------- | ------------------- | ------------------- |
| DWORD | 4 | 4 | Unsigned long |
| HMODULE | 4 | 8 | Pointer/handle to a module |
| FARPROC | 4 | 8 | Pointer to a function |
| LPSTR | 4 | 8 | Pointer to a character string |
| LPCWSTR | 4 | 8 | Pointer to a Unicode string |

- The compiler determines the actual size of the allocated storage for the basic data types which, along with the endian order, is dependent on the underlying hardware. Below we will explore how the C programming language in particular provides mechanisms to combine data types into data structures.

### Abstract Data Types
Abstract data types provide models for both the data and the operations performed on the data These models are independent of any particular programming language and are not concerned with the details of the particular data being stored.

**Arrays**: The simplest mechanism for aggregating data is the *one-dimensional array*. This is a collection of `<index, element>` pairs, in which the elements are of a single data type. The data type of the stored elements is then typically referred to as the *array type*. An array type is fixed when an instance of the array is created, which subsequently binds the number of elements that can be stored. The individual elements of the array can be accessed by specifying an array index that maps to an element's position within the array.

*One Dimensional Char Array Example*:
- Index:   `0 1 2 3 4`
- Element: `A B C D E`
  - To refer to the second element in this array (B), assuming it was named `letters`, you would use: `letters[1];`.

In C specifically, an array is referenced by the memory address of the first element, which is also known as the array's *base address*. The subsequent elements can be accessed as an offset from the base address. In a mathematical sense, assuming that an array is stored at base address *x* and is storing elements of size *s*, you can calculate the address of the element at index *I* using the following equation: `Address(I) = X + (I * S)`. This characteristic is typically referred to as *random access* because the time to access an element does not depend on what element you are accessing.

**Bitmaps**: An array variant used to represent sets, also known as the *bit vector* or *bit array*. In this instance, the index represents a fixed number of continuous integers, and the elements store a boolean value [1,0]. Within memory analysis, bitmaps are typically used to efficiently determine whether a particular object belongs to a set (that is, allocated vs. free memory, low versus high priority, and so on). They are stored as an array of bits, known as a map, and each bit represents whether one object is valid or not. Using bitmaps allows for representation of eight objects in one byte, which scales well to large data sets. For example, the Windows kernel uses a bitmap to maintain allocated network ports. Network ports are represented as an unsigned short, which is 2 bytes, and provides 65535 possibilities. This large number of ports is represented by a 65535-bit bitmap. Essentially, if a 1 is set to a value in the bitmap, then the corresponding port number is considered to be in use.

**Records**: a mechanism used for aggregating data which, unlike an array that requires elements that need to consist of the same data type, a record can be made up of different data types. It is composed of a collection of fields, where each field is specified by `<name, element>` pairs. Each field is also commonly referred to as a member of the record. Just like arrays, records are fixed in size after their initial creation. Specifying the particular member name, which acts as a key or index, accesses the elements of a record. A collection of elements can be combined within a record to create a new element. Similarly, it is also possible for an element of a record to be an array or record itself. An example of a record can be a network connection composed of four members that describe its characteristics: *id*, *port*, *addr*, and *hostname*. Each if this is a different data type but can be collected in a record data structure for better organization.

In the C programming language, records are implemented using structures. A structure enables a programmer to specify the name, type, and order of the members. For example:
```
struct Connection {
  short id;
  short port;
  unsigned long addr;
  char hostname[32];
}
```
C-style structures are some of the most important data structures encountered when performing memory analysis. After you have determined the base address of a particular structure, you can leverage the definition of the structure as a template for extracting and interpreting the elements of the record. Occasionally a compiler might add padding to certain fields of the structure stored in memory. The compiler does this to preserve the alignment of fields and enhance CPU performance.

**Strings**: these are often considered a special case of an array, in which the stored elements are constrained to represent character codes taken from a predetermined character encoding. Similar to an array, a string is composed of a collection of `<index, element>` pairs. Programming languages often provide routines for string manipulation, which may change the mappings between indices and elements. Unlike static data structures like arrays and records, a string can contain a variable length sequence of elements that may not be known when an instance is created. To do this, strings must provide a mechanism for determining the length of the stored collection.

A good example is the C-style string which provides an implementation that is very similar to how the C programming language implements arrays. Specifically, C-style strings that are of type *char* and encoded using ASCII character encoding assigns a 7-bit numerical value (almost always stored in a full byte) to the characters in American English. The major difference between a C-style string and an array is that C-style strings implicitly maintain the string length. This is accomplished by demarcating the end of the string with an embedded string termination character. In the case of C-style strings, the termination character is the ASCII NULL symbol, which is `0x00`. You can calculate the length of the string by determining the number of characters between the string's starting address and the termination character.

Windows has its own alternative string implementation that may be encountered during memory forensics. It is referred to as the `_UNICODE_STRING` implementation because that is the name of its supporting data structure. As its name suggests, this implementation uses UTF-16 UNICODE to encode its characters instead of ASCII by allowing elements to be larger than a single byte which allows for characters larger than those found in American English. By using UTF-16 UNICODE, characters are stored as either 2 or 4 byte values. The `_UNICODE_STRING` does not require a terminating character but instead stores the length explicitly. A `_UNICODE_STRING` is implemented using a structure that stores the length, in bytes, of the string (Length), the maximum number of bytes that can be stored in this particular string (MaximumLength), and a pointer to the starting memory address in which the characters are stored (Buffer).

Strings are an extremely important component of memory analysis, because they are used to store textual data (passwords, process names, filenames, and so on). During just about every incident, you will extract strings from any suspicious file you come across for evidence of malicious purposes. Normally when strings are extracted, they are searched until their terminating character. However, there can be occasional issues such as with systems that leverage paged virtual memory where strings that cross a page boundary to a page that is no longer memory resident which will require special processing or heuristics to determine the actual size of the string. `_UNICODE_STRING` implementations present a unique challenge since they only contain metadata for a string, (that is, its starting virtual memory address, size in bytes, and so on). Thus if you cannot perform virtual address translation, then you cannot locate the actual contents of the strings. Likewise, if you find the contents of a string through other means, you may not be able to determine its appropriate length, because the size metadata is stored separately.

**Linked Lists**: An abstract data type commonly used for storing a collection of elements. Linked-lists provide a flexible structure that can efficiently support dynamic updates and is unbounded with respect to the number of elements that it can store. A linked-list is intended to be more efficient for programs that need to frequently manipulate the stored collection by adding, removing, or rearranging elements. This added efficiency is accomplished by using links to denote relationships between elements and then updating those links as necessary. The first element of the list is commonly referred to as the head and the last element as the tail. Following the links from the head to the tail and counter the number of elements can determine the number of elements stored in a linked-list.
  - *Singly Linked List*: Each element is connected by a single link to its neighbor and the list can be traversed in only one direction. Inserting new elements and deleting elements from the list requires only a couple of operations to update the links. C implementations of this data structure will typically allocate and deallocate the memory to store elements as needed. As a result, you cannot assume that the elements will be stored contiguously in memory. Each element in the list is stored separately, and links are maintained to the neighboring elements. In some implementations, the links are stored embedded within the element (internal storage). In other implementations, the nodes of the linked-list contain links to the neighboring nodes and a link to the address in memory where the element is being stored (external storage). In either case, you implement the links between nodes by using pointers that hold the virtual memory address of the neighboring node. Thus, to access an arbitrary element of the list, you must traverse the linked list by following the pointers sequentially through the virtual address space.
  - *Doubly Linked List*: It is also possible to create a doubly linked list in which each element stores two pointers: one to its predecessor in the sequence and the other to its successor. Thus, you can traverse a doubly linked list both forward and backward.
  - *Circular Linked List*: A linked-list implementation used frequently in the Linux kernel. It is called a circular linked list because the final link is stored with the tail referring to the initial node in the list. This is useful for lists in which the ordering is not important. A circular linked list is traversed by starting at an arbitrary list node and stopping when the list traversal returns to that node.
  - *Embedded Doubly Linked Lists*: Often encountered when analyzing process accounting for Windows. It is known as "embedded" because it leverages internal storage to embed a `_LIST_ENTRY64` data structure within the element being stored. The `_LIST_ENTRY64` data structure contains only two members: a pointer to the successor's embedded `_LIST_ENTRY64 (Flink)` and a pointer to the predecessor's embedded `_LIST_ENTRY64 (Blink)`. Because the links store the addresses of other embedded `_LIST_ENTRY64` structures, you calculate the base address of the containing element by subtracting the offset of the embedded `_LIST_ENTRY64` structure within the element. Unlike the circular linked list, this implementation uses s separate `_LIST_ENTRY64` as a *sentinel node*, which is used only to demarcate where the list begins and ends.
  - *Lists in Physical and Virtual Memory*: When analyzing memory, you frequently encounter a variety of linked-list implementations. Unfortunately, you cannot determine whether the data you find is actually a *current* member of a list from physical address space analysis alone. You cannot determine an ordering for the list, nor can you use the stored links to find neighboring elements. On the other hand, physical address space analysis does enable you to potentially find elements that may have been deleted or surreptitiously removed to thwart analysis. Dynamic data structures, such as linked lists, are a frequent target of malicious modifications because they can be easily manipulated by simply updating a few links. Virtual memory analysis can be used to translate the virtual address pointers and traverse the links between nodes.

**Hash Tables**: Data structures that are often used in circumstances that require efficient insertions and searches where the data being stored is in `<key, element>` pairs. Hash tables are used throughout operating systems to store information about active processes, network connections, mounted file systems, and cached files. A common implementation encountered during memory analysis involves hash tables composed of arrays of linked-lists, otherwise known as *chained overflow hash tables*. Linux uses a chained overflow hash table (known as the process ID hash table) to associate process IDs with process structures.

**Trees**: Data structures used by Operating Systems when performance is critical. Hierarchical (rooted) trees are composed of a set of nodes used to store elements and a set of links used to connect the nodes. Each node also has a key used for ordering the nodes. One node is demarcated as the *root*, and the links between the nodes represent the hierarchical structure through parent-child relationships. Any node that does not have any children is referred to as a *leaf*. Any node in a tree and all its descendants form a *subtree*. A fundamental property of a hierarchical tree is that it does not possess any cycles. As a result, a unique path exists between any two nodes found in the tree.

One of the most important operations to be performed on a tree, especially during memory analysis, is *tree traversal*, which is the process of visiting the nodes of the tree to extract a systematic ordering. An ordering is used to determine how the edges to the children should be processed. The different techniques are classified based on the order in which the nodes are visited. The three most frequently encountered orderings are *preorder*, *inorder*, and *postorder*. Using preorder traversal, you first visit the current node and then visit the subtrees from left to right. Inorder traversal involves visiting the left subtree, then the current node, and finally the remaining subtrees from left to right. During postorder traversal, you visit each subtree from left to right and then visit the current node.

C programming implementations of trees have many of the same characteristics as linked lists. The major differences are the number and types of links maintained between nodes and how those links are ordered. The links in trees, who's nodes have memory for them dynamically allocated and deallocated as needed, are implemented as direct edges in which each node maintains pointers to the virtual memory addresses of related nodes.

Analyzing trees in memory also shares similar challenges faced during the analysis of linked lists. For example, physical memory analysis offers the potential to find instances of stored or previously stored elements scattered throughout memory, but does not have the context to discern relationships between those elements. However, virtual memory analysis can be used to traverse the tree to extract node relationships and stored elements. In the case of an ordered tree, if you know the traversal order or can discern it from the field names, you can extract the ordered list of elements. By combining the overall structure of the tree with the ordering criteria, you may even be able to discern information about how the elements were inserted or removed from the tree as it evolved over time.  
***

## Memory Acquisition

Memory Acquisition (i.e., *capturing*, *dumping*, *sampling*) involves copying the contents of volatile memory to non-volatile storage. In this section we will cover how memory acquisition tools work to better understand the acquisition process and how it could go wrong. Ultimately, you need to collect facts about the target machine to determine the best tools to use to perform a memory extraction. If the target system is a virtual machine (VM) you may be able to use the capabilities a hypervisor provides for pausing, suspending, taking a snapshot, or using introspection. If the target system is "bare metal", you need to determine what kind of state the system is running in. If it's hibernating or powered down, the *current* state of memory is not volatile. But, in many cases, *recent* volatile data may have been written to more persistent storage devices such as the hard disk. These alternate sources of data include hibernation files, page files, and crash dumps. Acquiring memory from non-volatile sources entails booting the target system with a live CD/DVD/USB to access its disk or make a forensic duplication of the disk image and mounting it (read-only, USE WRITE BLOCKERS!!!) from your analysis workstation.

A running system provides the opportunity to acquire the *current* state of volatile memory but you'll need administrator-level privileges. In these cases, you can use a software-based utility. Otherwise, it might be necessary to use a privilege escalation exploit, (or in the case of Linux-based routers like MikroTiks using an exploit to gain access to the underlying shell in the first place), or by brute force password guessing. Remember that these techniques might modify the state of the target you seek to analyze and you should know what elements of the target environment are affected BEFORE using them.

Another option is hardware-assisted acquisition. Typically this involves Direct Memory Access (DMA) using a technology such as Firewire, Thunderbolt, ExpressCard, or PCI. The downside to this is that if the target doesn't already have the required hardware installed, it will have to be powered down to install the various devices hardware-assisted methods normally require.

### The Risk Of Acquisition

Extracting the contents of physical memory is not without its risks. Below are some of the ways in which memory acquisition can lead to system instabilities and evidence corruption:

**Atomicity**: An *atomic operation* is one that appears (to the rest of the system) to complete instantaneously, without interruption from concurrent processes. Memory acquisition is *not* an atomic operation, because the contents of RAM are constantly changing, even on an idle system and *especially* during the acquisition process. Due to the constantly shifting nature of RAM, you might start and finish an extraction in which particular pages are captured after a critical operation began but before it finished, leading to a corrupted memory dump that analysis tools cannot process. If this happens be sure to obtain several extractions at different times and test all of them to see if it was just that one capture, or the tool itself is unable to recognize the OS specific elements of the target machine.

**Device Memory**: Physical memory is a logical addressing scheme that permits disparate motherboard resources to be accessed (or "addressed") in a uniform manner. On x86/x64-based computers, the firmware (BIOS) provides a physical memory map to the OS with different regions marked as reserved for use by the firmware by the ISA or PCI busses, or by varoius motherboard devices. These regions are often referred to as *device-memory regions*. When looking at the layout of physical memory, these device-memory regions will appear as holes and are the reason, for example, on 32-bit systems with less than 4GB of memory, the total amount of memory available to the OS will appear slightly less than the advertised capacity of the RAM chips. Windows provides a function known as `MmGetPhysicalMemoryRanges()` that omits the ranges of physical memory that are reserved by devices in order to avoid the device-memory regions.

Inadvertently reading from one of these reserved regions can be dangerous. Depending on the nature of the device being accessed, reading from a physical address within the region may obtain data stored at that location or *alter the state of the device you're accessing.* There are, for example, physical addresses that are mapped to device registers that can change the state of the device drivers or firmware that depend on the values in those registers, ultimately causing the system to freeze. This freezing or hanging is especially common when addresses occupied by the video chipset, *High Precision Event Timer (HPET)*, or obscure, legacy PCI devices are read. As an additional challenge, most of these devices are not designed to accommodate simultaneous access from more than one processor at a time.

Regardless of the risks associated with extracting from these regions, the evidence contained within can be highly valuable. For example, data in these regions can contain the real mode interrupt vector table (IVT) with artifacts left by firmware based rootkits. You can also find evidence of BIOS rootkits that inject code at the top of real mode memory. Additionally, it is possible to use the CMOS region to change the boot order and the IOAPIC indirect access registers to re-route interrupts.

**Cache Coherency**: Modern processors are designed with one or more internal memory caches to improve performance. A page table entry may be programmed with different memory cache attributes (non-cached, cached, write-combined) that determine the way in which the processor accesses a physical memory page. These processors have a documented design limitation: They are not designed to accommodate the simultaneous mapping of the same physical address with multiple cache attributes. Doing so might lead to undefined behavior on the part of the processor, including, but not limited to, translation lookaside buffer (TLB) corruption and corruption of the data at the specified memory address.

*Cache coherency* is one of the major reasons why Microsoft cautions Windows driver developers not to map physical pages unless their driver has directly allocated the memory through an API that guarantees no other system component has mapped the same memory range with a different caching type.  

**Software Acquisition Tools**: All software-based acquisition tools follow a similar protocol to acquire memory. In particular, these tools work by loading a kernel module that maps the desired physical addresses into the virtual address space of a task running on the system. At this point, they can access the data from the virtaul address space and write it to the requested non-volatile storage. Acquisition software has two ways to make this virtual-to-physical address mapping occur:
  1. The approach that most, if not all, commercially available tools utilize involves using an operating system API to create a page table entry. The typical functions include: `ZwMapViewOfSection` (on `\Device\PhysicalMemory`), `MmMapIoSpace`, `MmMapLockedPagesSpecifyCache`, and `MmMapMemoryDumpMdl`.
  2. A second possible approach uses other OS APIs to allocate an empty page table entry and manually encode the desired physical page into the page table entry.
There are substantial risks associated with either approach. For example, the aforementioned APIs for mapping physical to virtual memory all have a common limitation in that none of them are *intended* for mapping pages that a driver does not own.  

**Memory Dump Formats**: Volatility uses *address space voting rounds* to automatically identify file formats of memory dumps. This feature was implemented with law enforcement style digital forensics in mind as the analyst may not always get a say in how the first responder retrieves a memory capture from a crime scene. Volatility also provides several plugins for exploring the metadata associated with many of the common file formats:

| Format | Plugin | Output |
| ------ | ------ | ------ |
| Crash dump | `crashinfo` | CPU registers, critical pointers and symbol locations, bug check codes, memory runs |
| Hibernation file | `hibinfo` | CPU registers, timestamps, OS Version, memory runs |
| HPAK | `hpakinfo` | Memory runs, page file name, compression enabled/disabled |
| Mach-o | `machoinfo` | Memory runs |
| VMware | `vmwareinfo` | Memory runs, VMX configuration, CPU registers, PNG thumb-nail screenshot |
| VirtualBox | `vboxinfo` | Memory runs, VirtualBox version, number of CPUs |

**Raw Memory Dump**: The most widely supported format among analysis tools. It does not contain any headers, metadata, or magic values for file type identification. The raw format typically includes padding for any memory ranges that were intentionally skipped (i.e. device memory) or that could not be read by the acquisition tool, which helps maintain spatial integrity (relative offsets among data).

**Windows Crash Dump**: The Windows crash dump file format was designed for debugging purposes. Crash dumps begin with a `_DMP_HEADER` or `_DMP_HEADER64` structure. The header identifies the major and minor OS version, the kernel DTB (`DirectoryTableBase`), the addresses of the active process and loaded kernel module list heads, and information on the physical memory runs. It also shows the bug check codes, which a debugger uses to determine the cause of the crash.
  - **NOTE**: To make a Windows crash dump compatible with Volatility, it must be a *complete* memory dump, not a *kernel* memory dump or a *small* dump.
Below are some ways to create a crash dump, however, not all methods are suitable for forensics purposes:
  - **Blue Screens**: A Windows system can be configured to create a crash dump when a Blue Screen of Death (BSoD) happens. If you want to try this out, use the Sysinternals tool `NotMyFault` to trigger a BSoD.
  - **CrashOnScrollControl**: Some PS/2 and USB keyboards have special key sequences that produce a crash dump. On server systems that don't have keyboards attached, you can use Non-Maskable Interrupts. However, typically these methods require pre-configuration of the registry and BIOS.
  - **Debuggers**: If you're attached to a target system using a remote kernel debugger (WinDBG), you can use the `.crash` or `.dump` commands It is convenient for debugging, but it rarely ever applies to a forensic scenario. You can also use LiveKD if you're debugging locally, but it typically requires pre-installing special software on the target system.
As you might expect, crash dumps come with issues in relation to forensics. They typically don't include device memory regions or the first physical page, which might contain a copy of the Master Boot Record (MBR) from disk and pre-boot authentication passwords. Furthermore, they can be subverted by malware that registers a bug check callback or by disabling access to the kernel debugger. Some systems might not even be able to create complete crash dumps due to size.

**Windows Hibernation File**: A hibernation file (`hiberfil.sys`) contains a compressed copy of memory that the system dumps to disk during the hibernation process. Hibernation files consist of a standard header (`PO_MEMORY_IMAGE`), a set of kernel contexts and registers such as CR3, and several arrays of compressed data blocks. The compression format pre-Windows 8 and Server 2012 was basic Xpress. Starting in Windows 8 and Server 2012, Microsoft started using the Xpress algorithm along with Huffman and LZ encoding. The `Signature` member of the `PO_MEMORY_IMAGE` header usually contains `hibr`, `HIBR`, `wake`, or `WAKE`. However in some cases, the entire header is zeroed out (this happens when the system resumes), which con prevent analysis of the hibernation file in most tools. In those cases, Volatility uses a brute force algorithm to locate the data it needs. When performing an analysis of the hibernation file with Volatility, remember that every time you run a command, you need to decompress certain segments. To save time, it is recommended to use the `imagecopy` command to decompress the entire memory dump once. The decompression converts the hibernation file into a raw memory dump that you can analyze without on-the-fly decompression.
  - In order to create a hibernation file, first enable hibernation in the kernel (`powercfg.exe /hibernate on`) and then issue a `shutdown /h` command to hibernate, (or just use the GUI options from the Start Menu).
  - If you are working with a forensic image of a target, the hibernation file will be located at: `C:\hiberfil.sys`.
  - **NOTE**: Before a system hibernates, the DHCP configuration (if any) is released and any active connections are terminated. As a result, networking data in hibernation files might be incomplete. Also, during this time, malware can remove itself from memory so that you're not able to detect its presence in the hibernation file.

**Virtual Machine Memory**: Memory can be acquired from a VM by either running acquisition tools within the VM itself or they can be obtained from the hypervisor. The latter option is less invasive and makes it harder for malicious code lurking on a VM to detect an analyst's presence. Below we will cover the two largest Virtual Machine platforms on the market: VMWare and VirtualBox. Be aware, pausing or suspending a VM is not without risks as active SSL/TLS connections cannot easily resume after being "frozen".
  - **VMWare**: If you are using a desktop product such as Workstation, Player, or Fusion, you just need to suspend/pause or create a snapshot of a VM which will result in a copy of the VM's memory being written to the host's file system, relative to the `.vmx` configuration. If you're using VMWare Server or ESXI, you can do this from a vSphere GUI console and retrieve a copy of the `.vmx` file from the SAN or NFS data store the server saves to. Depending on the VMWare product and how the memory dump was created you might need to recover more than one file for analysis. Sometimes the VM's memory is entirely contained in a single `.vmem` file (the raw schema). In other instances you might get a `.vmsn` (snapshots) or `.vmss` (saved state), which are proprietary file formats containing memory and metadata (the combined schema). Lately, VMWare often creates a `.vmem` and one of the structured metadata files (the split schema). The bottom line is to collect all files with the extensions: `.vmem`, `.vmsn`, and `.vmss`.
  - **VirtualBox**: VirtualBox does not automatically save a full RAM dump to disk when a VM is suspended or paused. Instead, you must create a memory dump using one of the following technqiues:
    - The `vboxmanage debugvm` commands. This method creates and ELF64 core dump binary with custom sections that represent the guest's physical memory.
    - Use the `--dbg` switch when starting a VM and then the `.pgmphystofile` command. This method outputs a raw memory dump.
    - Use the VirtualBox Python API (`vboxapi`) to create your own memory dumping utility.

### Volatile Memory on Disk

Volatile data is often written to non-volatile storage as a matter of normal system operation, such as during hibernation and paging. Below we will discuss recovering data by using the Hibernation File and the Paging File. (If you haven't figured it out yet this is written from a Windows analysis point-of-view).

**Recovering the Hibernation File**: If available, a system's hibernation file will exist at `C:\hiberfil.sys`. If you have managed to get a raw image of the disk (for example, `image.dd`) you need to first identify the starting sector of the NTFS partition. To do this, you can use the `mmls` tool in the following mannger:
  - `mmls image.dd`
In the output, look at the description column and look for the line that contains `NTFS` which will indicate the boot sector. In the Start column there will be a number after the last 0 which will be the offset number from the start of the memory dump, (for example, 2048). This is the value you will provide to the Sleuth Kit tool `fls` to pull out the location of the hibernation file:
  - `fls -o 2048 image.dd | grep hiber`
This will produce a line of output containing the MFT number of the hibernation file, (for example, 36218). Finally, you can supply the number to the `icat` command to extract the file's contents:
  - `icat -o 2048 image.dd 36218 > /root/work/hiberfil.sys`

**Recovering the Page File(s)**: Typically the primary Windows page file is located at: `C:\pagefile.sys`, however, Windows systems can have up to 16 page files. In order to find all the page files, you can query the `SYSTEM` registry, for example:
  - `reglookup -p "ControlSet001/Control/Session Manager/Memory Management" -t MULTI_SZ /output/location/system`
This will output various file locations that will be the places you can find the system's various page files. In most cases it will just be the `C:\pagefile.sys` file, however, it is important to gather up all the page files to get a complete picture of the pieces of volatile memory written to disk. You can now recover the page file(s) using the same method used to grab the hibernation file:
  - `fls -o 2048 image.dd | grep pagefile`
Use the MFT value listed in the output just like before:
  - `icat -o 2048 image.dd 58981 > /root/work/pagefile.sys`
